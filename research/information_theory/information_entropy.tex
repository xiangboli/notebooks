\documentclass{article}
\title{Information Theory Basics}
\date{\today}
\author{Xiangbo Li}

\begin{document}
\maketitle

\section{Information Defintion}
\begin{equation}
	I = \log(1/p) = -\log(p)
\end{equation}

Example:
\begin{itemize}
	\item{\textbf{One flip of a fair coin}: Before the flip, there are two equally probable choices: heads or tails. After the flip,
	weâ€™ve narrowed it down to one choice. Amount of $information = log2(2/1) = 1$ bit.}
	\item{\textbf{Roll of two dice}: Each die has six faces, so in the roll of two dice there are 36 possible combinations for
	the outcome. Amount of $information = log2(36/1) = 5.2$ bits.}
\end{itemize}

\section{Entropy}
Now that we know how to measure the information contained in a given event, we can quantify the expected information in a set of 
possible outcomes. Specifically, if an event $i$ occurs with probability $p_i$, $1 \leq n \leq N $out of a set of $N$ events, then the average or expected 
information is given by

\begin{equation}
	H(p_1, p_2, ..., p_N) = \sum_{i=1}^np_i\log(1/p_i)
\end{equation}

H is also called the entropy (or Shannon entropy) of the probability distribution.  
It is often useful to think of the entropy as the average or expected uncertainty associated with this set of events. Shannon proved that the expected
code length of any decodable code cannot be smaller than the entropy, H, of the underlying probability distribution over the symbols.

\end{document}
